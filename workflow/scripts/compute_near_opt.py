# SPDX-FileCopyrightText: 2022 Koen van Greevenbroek & Aleksander Grochowicz
#
# SPDX-License-Identifier: GPL-3.0-or-later

"""Provides functionality to compute near-optimal feasible spaces.

For a description of the algorithm implemented in this script, see the
README.

"""

import copy
import logging
import multiprocessing
import os
import time
from collections import OrderedDict
from multiprocessing import Manager, Queue, get_context
from pathlib import Path
from typing import Collection, List

import numpy as np
import pandas as pd
import pypsa
from _helpers import configure_logging
from geometry import (
    ch_centre,
    facet_normals,
    filter_vectors,
    filter_vectors_auto,
    lhc_random_hypersphere_sampler,
    uniform_random_hypersphere_sampler,
)
from scipy.spatial import ConvexHull
from utilities import get_basis_values, solve_network_in_direction


def compute_near_opt(
    n: pypsa.Network,
    basis: OrderedDict,
    mga_space: pd.DataFrame,
    obj_bound: float,
    conv_method: str,
    direction_method: str,
    direction_angle_sep: float,
    conv_eps: float,
    conv_iter: int,
    max_iter: int,
    debug_dir: str,
    cache_dir: str,
    num_parallel_solvers: int,
    qhull_options: str = None,
    angle_tolerance: float = 0.1,
) -> Collection[np.array]:
    """Compute the near-optimal feasible space of `n`, cut off at eps.

    Implements an algorithm which iteratively solves the LP associated
    with `n` using different objective functions in order to 'probe'
    the near-optimal feasible space and find an approximate explicit
    description.

    We consider a projection of the entire feasible space of `n` onto
    the lower-dimensional space spanned by a selection of decision
    variables (or sums of decision variables). Specifically, the
    projection is the span of the vectors given by the `basis`
    argument. The elements of this collections are assumed to be
    linear expressions generated by `pypsa.linopt.linexpr`.

    The near-optimal feasible space is defined as the feasible space
    of `n` with the additional constraint 'c.x <= (1 + `eps`) * OPT',
    where c is the objective function and OPT the optimal value.

    A dictionary of PyPSA-Eur configuration (containing in particular
    the `solving` section) is assumed to be attached to `n` as
    `n.config`.

    The return value is a collection of points given in the basis
    specified by `basis`. The convex hull of these points is an
    approximation of the near-optimal feasible space of `n`.

    Various convergence criteria for the algorithm can be chosen. The
    first is convergence by volume; when the volume in successive
    iterations does not increase more than `conv_eps` the algorithm is
    terminated. The second method terminates when the distance between
    the Chebyshev centers of successive iterations drops below
    `conv_eps` (Note the different role and unit of `conv_eps` for
    different convergence methods.). The algorithm terminates after
    `conv_iter` iterations.

    Parameters
    ----------
    n : pypsa.Network
    basis : OrderedDict
        A basis on which to project the feasible space of `n`. The
        keys being the decision variables with values given in the
        format produced by `pypsa.linopt.linexpr`.
    mga_space : pd.DataFrame
        Points generated from the MGA iterations to compute the
        initial convex hull approximation.
    obj_bound : float
        Upper bound for costs in near-optimality constraint.
    conv_method : str
        Convergence method. Can be 'volume' or 'centre'.
    direction method : str
        Method for choosing directions. Can be one of 'facets',
        'random-uniform', 'random-lhc', 'maximal-centre' or
        'maximal-centre-then-facets'.
    direction_angle_sep : float
        Initial angle threshold for direction generation. Depending on
        `direction` in the configuration, this threshold can decrease
        automatically down to `angle_tolerance`.
    conv_eps : float
        Convergence threshold in percent.
    conv_iter : int
        Number of iterations for which the convergence criterion must
        be below `conv_eps` for the algorithm to terminate.
    max_iter : int
        The maximum number of iterations after which the algorithm is
        termined regardless of convergence.
    debug_dir : str
        Directory where the debug files (networks, volume, centre,
        radius, probed directions) should be saved.
    cache_dir : str
        Directory where the cache files (config, points, probed
        directions) should be saved. If points and probed directions
        from previous runs with the same configuration exist, these
        are being used.
    num_parallel_solvers : int
        The number of parallel processes to use.
    qhull_options : str
        Options for qhull, e.g. for numerical stability or to avoid
        certain degeneracies.
    angle_tolerance : float
        Minimal angle threshold for filtering.

    Returns
    -------
    Collection[np.array]
        A collection of points in the basis of `basis`, whose
        convex hull is an approximation of the near-optimal feasible
        space of the given model.

    """
    # Start by validating some of the arguments.
    # Check that the conv_method is defined.
    if conv_method not in ["volume", "centre"]:
        raise ValueError("'conv_method' argument should be 'volume' or 'centre'.")
    # Check that not too many parallel processes are called for.
    if num_parallel_solvers > max_iter:
        raise ValueError("Argument `num_parallel_solvers` is larger than `max_iter`")

    # Make sure that the debug- and cache directories exist.
    Path(debug_dir).mkdir(parents=True, exist_ok=True)
    Path(cache_dir).mkdir(parents=True, exist_ok=True)

    # Work on a copy of n so we do not modify the argument of this function.
    m: pypsa.Network
    m = copy.deepcopy(n)

    # Load the points generated during MGA. This dataframe is indexed
    # starting at -1, with the '-1'-th point being the optimum
    # solution. The indices of the other points align with the indices
    # of the corresponding probed directions (below).
    points: pd.DataFrame = mga_space

    # Initialise the probed_directions with the directions used for MGA.
    probed_directions: List[np.array]
    probed_directions = list(np.eye(len(basis))) + list(-np.eye(len(basis)))

    # Initialise a Dataframe for debug data for each iteration.
    iteration_data = pd.DataFrame(columns=[*points.columns, "radius", "volume"])

    # If there are results in the cache from previous runs with the
    # same configuration, load those.
    (
        previous_points,
        previous_directions,
        previous_iteration_data,
        num_iters,
    ) = reuse_results(cache_dir, debug_dir, basis)

    # If we found any previous iterations (when `num_iters` is greater
    # than 0), we can just pick up where we left off.
    if num_iters > 0:
        logging.info(f"Found {num_iters} previous iterations.")
        points = previous_points
        probed_directions = previous_directions
        iteration_data = previous_iteration_data

    # Using the MGA runs, we have already computed the possible ranges
    # that we can obtain through different directions. Lest we have
    # different orders of magnitude and little symmetry, we scale all
    # points to obtain something resembling an orthoplex. This should
    # make the computations of the convex hull (also numerically)
    # easier and will influence the iterations of centre and radius,
    # but not the eventual values of those (however, possibly when the
    # convergence criteria are activated).
    scaling_ranges = mga_space.max() - mga_space.min()
    # Check that we do not have any degenerate ranges. The value of 1.0
    # is somewhat arbitrary.
    if not (scaling_ranges > 1.0).all():
        raise RuntimeError("After MGA, the near-optimal space is degenerate.")
    scaled_points = points / scaling_ranges
    scaled_hull = ConvexHull(
        scaled_points.to_numpy(), incremental=True, qhull_options=qhull_options
    )

    # If no previous iterations were found, initialise the first row
    # of the `iteration_data` DataFrame.
    if num_iters == 0:
        centre, radius, _ = ch_centre(scaled_hull)
        iteration_data.loc[-1] = np.hstack((centre, radius, scaled_hull.volume))

    # Prepare the generator of directions to probe.
    if direction_method == "random-uniform":
        # Uniformly random directions.
        sampler = uniform_random_hypersphere_sampler(len(basis))
        dir_gen = filter_vectors_auto(
            sampler,
            init_angle=direction_angle_sep,
            initial_vectors=probed_directions,
            min_angle_tolerance=angle_tolerance,
        )
    elif direction_method == "random-lhc":
        # Random directions from Latin hypercube sampling.
        sampler = lhc_random_hypersphere_sampler(len(basis))
        dir_gen = filter_vectors_auto(
            sampler,
            init_angle=direction_angle_sep,
            initial_vectors=probed_directions,
            min_angle_tolerance=angle_tolerance,
        )
    elif direction_method == "facets":
        # Directions are normal vectors to the largest facets.
        dir_gen = large_facet_directions(
            scaled_hull,
            probed_directions,
            direction_angle_sep,
            autodecrease=True,
            min_angle_tolerance=angle_tolerance,
        )
    elif direction_method == "maximal-centre":
        # Directions are normals to hyperplanes touched by largest centre ball.
        dir_gen = touching_ball_directions(
            scaled_hull, probed_directions, angle_tolerance
        )
    elif direction_method == "maximal-centre-then-facets":
        # Directions are first picked by "maximal-centre", then
        # "facets".
        dir_gen = maximal_centre_then_facets(
            scaled_hull, probed_directions, direction_angle_sep
        )
    else:
        raise ValueError("No mode of choosing directions defined.")

    # Set up a queue that can be shared among different processes.
    manager = Manager()
    queue = manager.Queue()
    results = []

    # Start a pool of worker processes. Set the child process start
    # method to spawn (as opposed to the Linux default "fork") in
    # order to avoid occasional queue deadlocks. This is also
    # supported by all platforms.
    with get_context("spawn").Pool(num_parallel_solvers) as pool:
        # Generate initial directions, and start solving in those directions.
        directions = []
        for i in range(num_parallel_solvers):
            try:
                d = next(dir_gen)
                if d is not None:
                    directions.append(d)
                    probed_directions.append(d)
                else:
                    logging.warning(
                        f"Could only generate {len(directions)} directions for"
                        f" {num_parallel_solvers} workers. Will try to generate more"
                        " after first iteration."
                    )
                    break
            except StopIteration:
                logging.warning(
                    "Ran out of directions to probe! Could only generate"
                    f" {len(directions)} directions for {num_parallel_solvers} parallel"
                    " solvers."
                )
                break
        for d in directions:
            args = (queue, m, d, basis, obj_bound)
            results.append(pool.apply_async(solve_worker, args))

        # Process solver results as they are put into the queue.
        while True:
            p = queue.get()
            if p is None:
                # In this case the last optimisation was unsuccessful;
                # this can happen sporadically due to, for example,
                # numerical issues.
                logging.info(
                    f"Iteration {num_iters} unsuccessful: ignoring results and repeating."
                )
            else:
                # This block is only executed if the last optimisation
                # result was successful.
                logging.info(f"Finished iteration {num_iters}.")
                num_iters += 1

                # Scale the values for easier computations of the convex hull
                # and proportions.
                sp = list(p.values()) / scaling_ranges

                # Add the point to the convex hull.
                scaled_hull.add_points([sp])

                # Compute new Chebyshev centre and radius. Include a sanity
                # check that the radius does not decrease!
                centre, radius, _ = ch_centre(scaled_hull)
                old_radius = iteration_data.radius.iloc[-1]
                if (old_radius - radius) / old_radius > 0.001:
                    logging.info("Radius decreased. Check what is going on?")

                # Log the newly found point (together with the direction
                # that generated it), both in cache and debug directories.
                # These are non-scaled values. Note that the direction is
                # already added at this point.
                points.loc[points.index[-1] + 1] = p
                for d in [cache_dir, debug_dir]:
                    points.to_csv(os.path.join(d, "points.csv"))
                    pd.DataFrame(probed_directions, columns=points.columns).to_csv(
                        os.path.join(d, "probed_directions.csv")
                    )

                # Also write additional information about the new centre
                # point, radius and volume to the debug directory. These
                # come from the scaled near-optimal space.
                iteration_data.loc[iteration_data.index[-1] + 1] = np.hstack(
                    (centre, radius, scaled_hull.volume)
                )
                iteration_data.to_csv(os.path.join(debug_dir, "debug.csv"))

                # Evaluate convergence criteria. We need at least 2
                # iterations to do this.
                if num_iters >= 2:
                    if conv_method == "volume":
                        # Get array of volumes.
                        volumes = iteration_data.volume.values
                        # Compute percentage differences between iterations.
                        conv_deltas_percent = (
                            100 * (volumes[1:] - volumes[:-1]) / volumes[:-1]
                        )
                    elif conv_method == "centre":
                        # Compute a list of distances between the centres
                        # from successive iterations.
                        centres = list(iteration_data[list(points.columns)].values)
                        centre_shifts = np.array(
                            [dist(x, y) for x, y in zip(centres[1:], centres[:-1])]
                        )
                        # Calculate the percentage these distances make up
                        # of the magnitude of the centre at each
                        # iteration.
                        norms = np.array([np.linalg.norm(c) for c in centres])
                        conv_deltas_percent = 100 * centre_shifts / norms[:-1]

                    # Log the latest delta percentage.
                    logging.info(
                        "Latest convergence criteria (percent):"
                        f" {conv_deltas_percent[-1]:.3}"
                    )

                else:
                    conv_deltas_percent = []

                # If needed, we pad the `conv_deltas_percent` list in
                # order to contain at least `conv_iter` elements.
                conv_deltas_percent = np.concatenate(
                    [
                        (conv_iter - len(conv_deltas_percent)) * [np.inf],
                        conv_deltas_percent,
                    ]
                )

                # End the approximation algorithm if we converge or reach
                # the maximum number of iterations.
                conv_crit = all(
                    [d < conv_eps for d in conv_deltas_percent[-conv_iter:]]
                )
                if conv_crit or (num_iters >= max_iter):
                    logging.info("Terminating pool.")
                    pool.terminate()
                    break

            # The following is executed regardless of whether the last
            # optimisation was successful or not.

            # Add additional jobs to the queue. Most of the time we
            # should only need to start _one_ additional worker (since
            # we just finished processing the results of another
            # worker), but it is possible that the pool has more than
            # one idle workers when it was not possible to generate
            # enough directions at an earlier stage (for example when
            # our space does not have enough facets yet right in the
            # beginning).

            num_idle_workers = num_parallel_solvers - len(
                [r for r in results if not r.ready()]
            )
            if num_idle_workers > 1:
                logging.info(
                    f"Trying to generate new directions for {num_idle_workers} idle"
                    " workers."
                )
            for i in range(num_idle_workers):
                # Try generating a new direction and give it to a
                # worker.
                try:
                    dir = next(dir_gen)
                    if dir is not None:
                        probed_directions.append(dir)
                        args = (queue, m, dir, basis, obj_bound)
                        results.append(pool.apply_async(solve_worker, args))
                    else:
                        # We've (possibly temporarily) ran out of
                        # directions. Can try again after the next
                        # iteration.
                        break
                except StopIteration:
                    # In this case we really cannot generate any more
                    # directions.
                    break

            # If the worker pool is now completely idle (i.e. all the
            # results are ready), that means we have completely run
            # out of directions to probe. In that case, we end the
            # approximation.
            if all([r.ready() for r in results]):
                # All processes are done.
                pool.close()
                break

    # Log the algorithm termination.
    logging.info(f"\nApproximation ended after {num_iters} iterations.\n")
    deltas_str = np.array_str(
        np.array(conv_deltas_percent[-conv_iter:]), precision=3, suppress_small=True
    )
    logging.info(f"Last {conv_iter} deltas (percent): " + deltas_str)
    if conv_crit:
        logging.info("Conclusion: converged.")
    elif num_iters >= max_iter:
        logging.info("Conclusion: reached maximum number of iterations.")
    else:
        logging.info("Conclusion: ran out of directions to probe.")

    # Now reverse the scaling to obtain non-scaled "real" values.
    scaled_points = points / scaling_ranges
    scaled_hull = ConvexHull(scaled_points.to_numpy(), qhull_options=qhull_options)
    scaled_vertices = scaled_hull.points[scaled_hull.vertices, :]
    vertices = scaled_vertices * scaling_ranges.values
    return vertices


def solve_worker(
    queue: Queue,
    n: pypsa.Network,
    dir: np.array,
    basis: OrderedDict,
    obj_bound: float,
) -> None:
    """Solve a network in a given direction and put the results in a queue.

    This function returns an extreme point of the reduce near-optimal
    feasible space of `n` in the direction `dir`.

    """
    # Log the start of this iteration. Note that we cannot really use
    # the `logging` package here since it is not process-safe, so we
    # just use a print statement. At least it lets the user know
    # what is going on.
    worker_name = multiprocessing.current_process().name
    dir_str = np.array_str(dir, precision=3, suppress_small=True)
    print(f"{worker_name}: Optimising in direction {dir_str}")

    # Do the optimisation in the given direction.
    t = time.time()
    r = copy.deepcopy(n)
    status, _ = solve_network_in_direction(r, dir, basis, obj_bound)
    solve_time = round(time.time() - t)
    print(f"{worker_name}: Finishing optimisation in {solve_time} seconds.")

    # Put the result in the result queue if the optimisation was
    # successful. If unsuccessful, put a None in the queue. This may
    # happen sporadically due to, for example, numerical issues. Note
    # that we do have to put _something_ in the queue, otherwise (and
    # if there is only one parallel process) the main program loop
    # will get stuck waiting for a result.
    if status == "ok":
        queue.put(get_basis_values(r, basis))
    else:
        queue.put(None)


def large_facet_directions(
    hull: ConvexHull,
    probed_directions: Collection[np.array],
    init_min_angle: float = 10.0,
    autodecrease: bool = False,
    min_angle_tolerance: float = 0.1,
):
    """Generate directions based on facets.

    In particular, for each iteration this generator sorts the normal
    vectors of the facets of `hull` by facet volume, and returns the
    normal vector of the largest facet. Normals that are close to any
    vector in `probed_directions` are filtered out. If `autodecrease`
    is set to True (default: False), then we decrease the angles by 20% to
    try to get more vectors. Then it is necessary to add a minimial angle,
    `min_angle_tolerance`.

    The arguments `hull` and `probed_directions` are taken as
    references and may change between each iteration of this
    generator.

    This generator never terminates, and instead returns None when it
    runs out of directions. This is so that it can "try again" after
    it failed to find a new direction, but the hull was updated in the
    mean time.

    Parameters
    ----------
    hull : ConvexHull
        Based on this convex hull, generate directions by its facets.
    probed_directions : Collection[np.array]
        Filter the directions based on this collection.
    init_min_angle : float
        First angle threshold for filtering before possible reductions.
    autodecrease : bool
        If True, automatically decrease the angle threshold by 20% if
        no directions can be found otherwise, unless the angle goes
        below `min_angle_tolerance`.
    min_angle_tolerance : float
        Minimal threshold for direction filtering. Below it, the
        direction generation ends, as we have exhausted the possible
        vectors.

    """
    a = init_min_angle
    while True:
        # Get the normals for the current hull. (Note that `hull` is
        # updated for every iteration.)
        normals = facet_normals(hull)
        try:
            # Filter out directions close to ones we have seen before,
            # and return the first one.
            yield next(
                filter_vectors(normals, angle=a, initial_vectors=probed_directions)
            )
        except StopIteration:
            if autodecrease:
                # If we ran out of directions here, decrease the
                # minimum allowed angle between probed directions
                # (until we reach an absolute minimum).
                a *= 0.8
                if a < min_angle_tolerance:
                    # At this point we seem to really have run out of directions.
                    yield None
                    continue
                logging.info(f"Decreasing allowed angle between directions to {a}.")
                continue
            yield None


def touching_ball_directions(
    hull: ConvexHull,
    probed_directions: Collection[np.array],
    angle_tolerance: float,
):
    """Compute normals of planes touched by the Chebyshev ball at the centre.

    This generator computes for each iteration the normals of the
    hyperplanes touching the largest centre ball of `hull`, and
    returns one of them. Normals which are too close to any vector in
    `probed_directions` are filtered out (defined by the threshold
    `angle_tolerance`).

    The arguments `hull` and `probed_directions` are taken as
    references and may change between each iteration of this
    generator.

    Note that we do not decrease the angles here as we already start
    with a very low threshold to exploit the touching directions precisely
    from the beginning.

    This generator never terminates, and instead returns None when it
    runs out of directions. This is so that it can "try again" after
    it failed to find a new direction, but the hull was updated in the
    mean time.

    Parameters
    ----------
    hull : ConvexHull
        Based on this convex hull, generate directions by its facets.
    probed_directions : Collection[np.array]
        Filter the directions based on this collection.
    angle_tolerance : float
        Threshold for filtering directions.

    """
    while True:
        # Compute the Chebyshev centre of `hull` and get the
        # constraints which are tight in the resulting LP. These
        # constraints are exactly the normal vectors of the
        # hyperplanes of `hull` touching the centre ball.
        _, _, tight_constraints = ch_centre(hull)
        # Just yield any of the normals that is not too close to
        # something we have tried before.
        try:
            yield next(
                filter_vectors(
                    tight_constraints,
                    angle=angle_tolerance,
                    initial_vectors=probed_directions,
                )
            )
        except StopIteration:
            yield None


def maximal_centre_then_facets(
    hull: ConvexHull,
    probed_directions: Collection[np.array],
    init_min_facet_angle: float = 10.0,
    angle_tolerance: float = 0.1,
):
    """Generate directions first from centre ball, then facets.

    For each iteration of this generator, it is first checked if there
    are any normal vectors of facets touched by the centre ball of
    `hull` which have not been probed yet. If such a vector is found,
    it is yielded. If not, the normal vectors of the largest unchecked
    facets are returned instead. Once these are exhausted, we reduce
    the angle threshold automatically by 20%.

    Parameters
    ----------
    hull : ConvexHull
        Based on this convex hull, generate directions by its facets.
    probed_directions : Collection[np.array]
        Filter the directions based on this collection.
    init_min_angle : float
        First angle threshold for filtering before possible reductions.
    min_angle_tolerance : float
        Minimal threshold for direction filtering. Below it, the
        direction generation ends, as we have exhausted the possible
        vectors.

    """
    a = init_min_facet_angle
    while True:
        # First try generating a direction using the centre ball. This
        # generator yields None if nothing was found.
        d = next(touching_ball_directions(hull, probed_directions, angle_tolerance))
        if d is not None:
            logging.info("Generated direction based on maximal-centre.")
            yield d
            continue

        # In case we did not find any new directions from the facets
        # touching the centre ball, go with normal directions to large
        # facets.
        d = next(large_facet_directions(hull, probed_directions, a))
        if d is not None:
            logging.info("Generated direction based on largest facet.")
            yield d
        else:
            # If we ran out of directions here, decrease the minimum
            # allowed angle between probed directions (until we reach
            # an absolute minimum).
            a *= 0.8
            if a < angle_tolerance:
                # At this point we really seems to have run out of directions.
                yield None
                continue
            logging.info(f"Decreasing allowed angle between directions to {a}.")


def dist(x: np.array, y: np.array) -> float:
    """Compute the Euclidean distance between x and y."""
    return np.linalg.norm(x - y)


def reuse_results(
    cache_dir: str, debug_dir: str, basis: OrderedDict
) -> (pd.DataFrame, Collection[np.array], pd.DataFrame, int):
    """Reuse results from cache directory, if any.

    This is based on hashing the configuration excluding iterations,
    conv_epsilon, conv_iterations from near_opt_approx.

    Parameters
    ----------
    cache_dir : str
        Directory where the cache files (config, points, probed
        directions) should be loaded from.
    debug_dir : str
        Directory where the debug files (networks, volume, centre,
        radius, probed directions) should be loaded from.
    basis : OrderedDict
        A basis on which to project the feasible space of the network.
        The keys being the decision variables with values given in the
        format produced by `pypsa.linopt.linexpr`.

    Returns
    -------
    pd.DataFrame
        A collection of points in the basis of `basis` that were
        previously generated with this configuration.
    Collection[np.array]
        Previously probed directions.
    pd.DataFrame
        Previous iteration data (centre, radius, volume).
    int
        Number of previous iterations with this configuration.
    """
    try:
        # Read points and directions files.
        points = pd.read_csv(os.path.join(cache_dir, "points.csv"), index_col=0)
        probed_directions = pd.read_csv(
            os.path.join(cache_dir, "probed_directions.csv"), index_col=0
        )
        previous_debug = pd.read_csv(os.path.join(debug_dir, "debug.csv"), index_col=0)
        probed_directions = list(probed_directions.values)
        # Calculate the number of iterations that were performed. Note
        # that `probed_directions` (and `points`) contain data from
        # MGA iterations, so we subtract the number of MGA iterations
        # (which is 2*`len(basis)`).
        previous_iters = max(0, len(probed_directions) - 2 * len(basis))
    except OSError:
        logging.info(
            "No previous runs with this configuration found. Start with num_iters = 0."
        )
        return None, None, None, 0

    try:
        # Also try to read previous debug information if possible.
        previous_debug = pd.read_csv(os.path.join(debug_dir, "debug.csv"), index_col=0)
    except (OSError, TypeError):
        logging.info("Could not find debug data, use only cached data.")
        previous_debug = None
    return points, probed_directions, previous_debug, previous_iters


if __name__ == "__main__":
    # Set up logging so that everything is written to the right log file.
    configure_logging(snakemake)

    # Disable logging from pypsa; it mostly just distracts for this script.
    pypsa_logger = logging.getLogger("pypsa")
    pypsa_logger.setLevel(logging.WARNING)

    # Load the network.
    n = pypsa.Network(snakemake.input.network)

    # Attach solving configuration to the network.
    n.config = snakemake.config["pypsa-eur"]

    # Load the points generated during MGA.
    mga_space = pd.read_csv(snakemake.input.mga_space, index_col=0)

    # Depending on the 'eps' wildcard, determine the cutoff of the
    # near-optimal feasible space.
    with open(snakemake.input.obj_bound, "r") as f:
        obj_bound = float(f.read())

    # Compute the near-optimal feasible space.
    near_opt = compute_near_opt(
        n,
        basis=snakemake.config["projection"],
        mga_space=mga_space,
        obj_bound=obj_bound,
        conv_method=snakemake.config["near_opt_approx"]["conv_method"],
        direction_method=snakemake.config["near_opt_approx"]["directions"],
        direction_angle_sep=snakemake.config["near_opt_approx"][
            "directions_angle_separation"
        ],
        conv_eps=float(snakemake.config["near_opt_approx"]["conv_epsilon"]),
        conv_iter=int(snakemake.config["near_opt_approx"]["conv_iterations"]),
        max_iter=int(snakemake.config["near_opt_approx"]["iterations"]),
        debug_dir=snakemake.log.iterations,
        cache_dir=snakemake.log.cache,
        num_parallel_solvers=snakemake.config["near_opt_approx"].get(
            "num_parallel_solvers", 1
        ),
        qhull_options=snakemake.config["near_opt_approx"].get("qhull_options", None),
        angle_tolerance=snakemake.config["near_opt_approx"].get("angle_tolerance", 0.1),
    )

    # Write the points defining the near-optimal feasible space to the
    # given output file.
    pd.DataFrame(near_opt, columns=mga_space.columns).to_csv(snakemake.output.near_opt)
